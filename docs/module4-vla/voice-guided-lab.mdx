# Voice-Guided Lab: Autonomous Assistant

This comprehensive lab integrates **Whisper, GPT-4, Isaac ROS, and Nav2** to build a complete voice-controlled autonomous assistant. The robot will understand spoken commands, reason about tasks, and execute multi-step plans.

## Lab Objectives

By the end of this lab, you will have built a system that can:

✅ Accept **voice commands** in natural language
✅ **Plan tasks** using GPT-4 reasoning
✅ **Navigate** autonomously using Nav2
✅ **Detect and manipulate** objects with Isaac ROS
✅ **Handle ambiguity** and request clarification
✅ **Provide feedback** via text-to-speech

## Prerequisites

- **Module 3 Complete**: Isaac Sim, Isaac ROS, Nav2
- **Module 4 Sections 1-3**: LLM planning, Whisper, multi-modal interaction
- **OpenAI API Key**: For GPT-4 and Whisper API
- **Microphone**: USB or built-in

## System Architecture

```
┌──────────────────────────────────────────────────┐
│               Voice Input (User)                 │
└────────────────────┬─────────────────────────────┘
                     ↓
┌──────────────────────────────────────────────────┐
│         Whisper Recognition Node                 │
│  - Captures audio from microphone                │
│  - Transcribes with Whisper                      │
└────────────────────┬─────────────────────────────┘
                     │ /voice_command (String)
                     ↓
┌──────────────────────────────────────────────────┐
│            GPT-4 Task Planner Node               │
│  - Breaks down command into steps                │
│  - Generates action sequence                     │
└────────────────────┬─────────────────────────────┘
                     │ /task_plan (TaskPlan)
                     ↓
┌──────────────────────────────────────────────────┐
│          Visual Grounding Node                   │
│  - Identifies objects with Isaac ROS             │
│  - Resolves "that object" references             │
└────────────────────┬─────────────────────────────┘
                     │ /grounded_objects (ObjectArray)
                     ↓
┌──────────────────────────────────────────────────┐
│           Action Executor Node                   │
│  - Executes navigation (Nav2)                    │
│  - Executes manipulation (MoveIt2)               │
│  - Provides TTS feedback                         │
└──────────────────────────────────────────────────┘
```

## Part 1: Setup Isaac Sim Environment

### Create Lab Scene

```python
# voice_guided_lab_scene.py
from omni.isaac.kit import SimulationApp
simulation_app = SimulationApp({"headless": False})

from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid, VisualCuboid
from omni.isaac.core.prims import GeometryPrim
import numpy as np

# Create world
world = World()
world.scene.add_default_ground_plane()

# Add humanoid robot (from Module 3)
from omni.isaac.core.utils.stage import add_reference_to_stage
robot_path = "/World/Humanoid"
add_reference_to_stage(
    usd_path="/path/to/humanoid.usd",
    prim_path=robot_path
)

# Create environment: Kitchen with objects
# Table
table = VisualCuboid(
    prim_path="/World/Table",
    position=[2.0, 0.0, 0.4],
    scale=[0.8, 0.6, 0.02],
    color=[0.6, 0.4, 0.2]
)

# Objects on table
red_cup = DynamicCuboid(
    prim_path="/World/RedCup",
    name="red_cup",
    position=[2.0, -0.2, 0.45],
    scale=[0.04, 0.04, 0.06],
    color=[1.0, 0.0, 0.0]
)

blue_plate = DynamicCuboid(
    prim_path="/World/BluePlate",
    name="blue_plate",
    position=[2.0, 0.2, 0.42],
    scale=[0.1, 0.1, 0.01],
    color=[0.0, 0.0, 1.0]
)

yellow_bowl = DynamicCuboid(
    prim_path="/World/YellowBowl",
    name="yellow_bowl",
    position=[1.8, 0.0, 0.44],
    scale=[0.06, 0.06, 0.04],
    color=[1.0, 1.0, 0.0]
)

# Locations for navigation
locations = {
    "kitchen": [2.0, 0.0, 0.0],
    "living_room": [0.0, 0.0, 0.0],
    "bedroom": [-2.0, 2.0, 0.0],
}

# Enable ROS 2 bridge
from omni.isaac.ros2_bridge import ROS2Bridge
ros_bridge = ROS2Bridge()

# Camera for object detection
from omni.isaac.core.utils.prims import create_prim
camera = create_prim(
    "/World/Camera",
    "Camera",
    position=[1.0, 0.0, 1.5],
    orientation=[0.0, 0.0, 0.0, 1.0]
)

# Run simulation
world.reset()

while simulation_app.is_running():
    world.step(render=True)

simulation_app.close()
```

## Part 2: Voice Recognition Node

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper
import pyaudio
import numpy as np
import threading

class VoiceRecognitionNode(Node):
    def __init__(self):
        super().__init__('voice_recognition')

        # Parameters
        self.declare_parameter('model_size', 'small')
        model_size = self.get_parameter('model_size').value

        # Load Whisper
        self.get_logger().info(f'Loading Whisper {model_size} model...')
        self.model = whisper.load_model(model_size)

        # Publisher
        self.command_pub = self.create_publisher(String, '/voice_command', 10)

        # Audio
        self.RATE = 16000
        self.CHUNK = 1024
        self.audio = pyaudio.PyAudio()

        # Start listening
        self.is_listening = True
        self.listen_thread = threading.Thread(target=self.listen_loop)
        self.listen_thread.start()

        self.get_logger().info('Voice Recognition Node ready. Speak commands...')

    def listen_loop(self):
        while self.is_listening and rclpy.ok():
            transcript = self.capture_and_transcribe(duration=3.0)

            if transcript.strip():
                self.get_logger().info(f'Heard: "{transcript}"')

                msg = String()
                msg.data = transcript
                self.command_pub.publish(msg)

    def capture_and_transcribe(self, duration):
        stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.RATE,
            input=True,
            frames_per_buffer=self.CHUNK
        )

        frames = []
        for _ in range(0, int(self.RATE / self.CHUNK * duration)):
            data = stream.read(self.CHUNK)
            frames.append(data)

        stream.stop_stream()
        stream.close()

        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
        audio_data = audio_data.astype(np.float32) / 32768.0

        result = self.model.transcribe(audio_data, fp16=False)
        return result["text"]

def main():
    rclpy.init()
    node = VoiceRecognitionNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

## Part 3: GPT-4 Task Planner Node

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from robot_interfaces.msg import TaskPlan, ActionStep
import openai
import json

class GPT4PlannerNode(Node):
    def __init__(self):
        super().__init__('gpt4_planner')

        # OpenAI setup
        self.declare_parameter('openai_api_key', '')
        openai.api_key = self.get_parameter('openai_api_key').value

        # Subscribers
        self.command_sub = self.create_subscription(
            String, '/voice_command', self.command_callback, 10
        )

        # Publishers
        self.plan_pub = self.create_publisher(TaskPlan, '/task_plan', 10)
        self.feedback_pub = self.create_publisher(String, '/tts_feedback', 10)

        # Available actions
        self.actions = [
            "navigate(location: str) - Move to location",
            "detect_objects() - Identify objects in scene",
            "pick(object: str) - Grasp object",
            "place(location: str) - Put down object",
            "say(message: str) - Speak message to user"
        ]

        # Known locations
        self.locations = ["kitchen", "living_room", "bedroom"]

        self.get_logger().info('GPT-4 Planner Node started')

    def command_callback(self, msg):
        command = msg.data
        self.get_logger().info(f'Planning for: "{command}"')

        # Generate plan
        plan = self.generate_plan(command)

        # Publish plan
        self.plan_pub.publish(plan)

        # Provide feedback
        feedback = String()
        feedback.data = f"I will {command}"
        self.feedback_pub.publish(feedback)

    def generate_plan(self, command):
        prompt = f"""
        You are a robot task planner.

        User command: "{command}"

        Available actions:
        {chr(10).join('- ' + action for action in self.actions)}

        Known locations: {', '.join(self.locations)}

        Generate a step-by-step plan. Output JSON:
        {{
            "steps": [
                {{"action": "navigate", "params": {{"location": "kitchen"}}}},
                {{"action": "detect_objects", "params": {{}}}},
                ...
            ]
        }}

        Be concise. Only include necessary steps.
        """

        response = openai.ChatCompletion.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": "You are a helpful robot assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )

        # Parse response
        plan_json = json.loads(response.choices[0].message.content)

        # Convert to ROS message
        task_plan = TaskPlan()
        task_plan.header.stamp = self.get_clock().now().to_msg()

        for step in plan_json['steps']:
            action_step = ActionStep()
            action_step.action_type = step['action']
            action_step.parameters = [f"{k}={v}" for k, v in step['params'].items()]
            task_plan.steps.append(action_step)

        return task_plan

def main():
    rclpy.init()
    node = GPT4PlannerNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

## Part 4: Visual Grounding Node

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray
from cv_bridge import CvBridge
import clip
import torch
from ultralytics import YOLO

class VisualGroundingNode(Node):
    def __init__(self):
        super().__init__('visual_grounding')

        # Load models
        self.detector = YOLO("yolov8n.pt")
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device="cuda")

        # CV Bridge
        self.bridge = CvBridge()

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.image_callback, 10
        )

        # Publisher
        self.detections_pub = self.create_publisher(
            Detection2DArray, '/grounded_objects', 10
        )

        self.latest_image = None

        self.get_logger().info('Visual Grounding Node started')

    def image_callback(self, msg):
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

    def ground_object(self, description):
        """Find object matching description"""
        if self.latest_image is None:
            return None

        # Detect objects
        results = self.detector(self.latest_image)[0]

        # For each detection, compute CLIP similarity
        best_match = None
        best_score = -float('inf')

        for box in results.boxes:
            x1, y1, x2, y2 = box.xyxy[0].int().tolist()
            crop = self.latest_image[y1:y2, x1:x2]

            # CLIP embedding
            image_input = self.clip_preprocess(Image.fromarray(crop)).unsqueeze(0).to("cuda")
            text_input = clip.tokenize([description]).to("cuda")

            with torch.no_grad():
                image_features = self.clip_model.encode_image(image_input)
                text_features = self.clip_model.encode_text(text_input)
                similarity = (image_features @ text_features.T).item()

            if similarity > best_score:
                best_score = similarity
                best_match = {"bbox": (x1, y1, x2, y2), "score": similarity}

        return best_match

def main():
    rclpy.init()
    node = VisualGroundingNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

## Part 5: Action Executor Node

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from robot_interfaces.msg import TaskPlan
from nav2_msgs.action import NavigateToPose
from geometry_msgs.msg import PoseStamped
from std_msgs.msg import String
from gtts import gTTS
import os

class ActionExecutorNode(Node):
    def __init__(self):
        super().__init__('action_executor')

        # Subscribers
        self.plan_sub = self.create_subscription(
            TaskPlan, '/task_plan', self.plan_callback, 10
        )
        self.tts_sub = self.create_subscription(
            String, '/tts_feedback', self.tts_callback, 10
        )

        # Action clients
        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

        # Predefined locations
        self.locations = {
            "kitchen": {"x": 2.0, "y": 0.0},
            "living_room": {"x": 0.0, "y": 0.0},
            "bedroom": {"x": -2.0, "y": 2.0},
        }

        self.get_logger().info('Action Executor Node started')

    def plan_callback(self, plan):
        self.get_logger().info(f'Executing plan with {len(plan.steps)} steps')

        for i, step in enumerate(plan.steps):
            self.get_logger().info(f'Step {i+1}/{len(plan.steps)}: {step.action_type}')

            if step.action_type == 'navigate':
                location = self.parse_param(step.parameters, 'location')
                self.navigate_to_location(location)

            elif step.action_type == 'detect_objects':
                self.detect_objects()

            elif step.action_type == 'pick':
                obj = self.parse_param(step.parameters, 'object')
                self.pick_object(obj)

            elif step.action_type == 'say':
                message = self.parse_param(step.parameters, 'message')
                self.speak(message)

    def navigate_to_location(self, location_name):
        if location_name not in self.locations:
            self.get_logger().error(f'Unknown location: {location_name}')
            return

        coords = self.locations[location_name]

        goal_msg = NavigateToPose.Goal()
        goal_msg.pose.header.frame_id = 'map'
        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()
        goal_msg.pose.pose.position.x = coords["x"]
        goal_msg.pose.pose.position.y = coords["y"]
        goal_msg.pose.pose.orientation.w = 1.0

        self.nav_client.wait_for_server()
        future = self.nav_client.send_goal_async(goal_msg)
        rclpy.spin_until_future_complete(self, future)

        self.get_logger().info(f'Navigated to {location_name}')

    def detect_objects(self):
        # Trigger object detection (already running in visual grounding node)
        self.get_logger().info('Detecting objects...')

    def pick_object(self, object_name):
        # Interface with MoveIt2 or gripper controller
        self.get_logger().info(f'Picking {object_name}')

    def speak(self, text):
        tts = gTTS(text=text, lang='en')
        tts.save("/tmp/response.mp3")
        os.system("mpg123 /tmp/response.mp3")

    def tts_callback(self, msg):
        self.speak(msg.data)

    def parse_param(self, params, key):
        """Extract parameter value from list"""
        for param in params:
            if param.startswith(f'{key}='):
                return param.split('=', 1)[1]
        return None

def main():
    rclpy.init()
    node = ActionExecutorNode()
    rclpy.spin(node)
    rclpy.shutdown()
```

## Part 6: Integration and Testing

### Launch All Nodes

```bash
# Terminal 1: Isaac Sim
python voice_guided_lab_scene.py

# Terminal 2: ROS 2 nodes
ros2 launch voice_guided_lab lab.launch.py
```

**lab.launch.py**:
```python
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='voice_guided_lab',
            executable='voice_recognition_node.py',
            parameters=[{'model_size': 'small'}]
        ),
        Node(
            package='voice_guided_lab',
            executable='gpt4_planner_node.py',
            parameters=[{'openai_api_key': 'your_key_here'}]
        ),
        Node(
            package='voice_guided_lab',
            executable='visual_grounding_node.py'
        ),
        Node(
            package='voice_guided_lab',
            executable='action_executor_node.py'
        ),
    ])
```

### Test Commands

**Test 1: Simple Navigation**
```
You: "Go to the kitchen"
Robot: "I will go to the kitchen" [navigates]
```

**Test 2: Object Retrieval**
```
You: "Bring me the red cup from the kitchen"
Robot: [navigates to kitchen, detects red cup, picks it up, returns]
```

**Test 3: Multi-Step Task**
```
You: "Clear the table in the kitchen"
Robot: [navigates to kitchen, detects objects on table, picks each, places in bin]
```

## Evaluation Metrics

### 1. Speech Recognition Accuracy

```python
test_commands = [
    "Go to the kitchen",
    "Pick up the red cup",
    "Place it on the table"
]

correct_transcriptions = 0
for command in test_commands:
    transcription = whisper_node.transcribe(command)
    if transcription.lower() == command.lower():
        correct_transcriptions += 1

accuracy = correct_transcriptions / len(test_commands)
print(f"Transcription accuracy: {accuracy * 100:.1f}%")
```

### 2. Task Completion Rate

- **Success**: Task completed as intended
- **Partial**: Task started but not completed
- **Failure**: Task not attempted or failed

### 3. Latency

Measure end-to-end latency:
- Voice capture → Transcription: ~500ms
- LLM planning: ~1-3s
- Execution start: ~500ms
- **Total**: 2-4 seconds from voice to action

## Summary

This lab demonstrates:

✅ **End-to-end VLA system** from voice to action
✅ **GPT-4 task planning** for complex instructions
✅ **Visual grounding** with CLIP and YOLO
✅ **ROS 2 integration** with Isaac Sim and Nav2
✅ **Multi-step task execution** with feedback

**Key takeaways**:
- Voice control enables natural human-robot interaction
- LLMs provide flexible task planning
- Multi-modal fusion (voice + vision) resolves ambiguity
- Modular ROS 2 architecture enables easy extension

## Next Steps

Congratulations! You've completed Module 4. Continue to **Module 5: Capstone Project** to integrate everything into a fully autonomous humanoid system!

---

## References

[^1]: Ahn, M., et al. (2022). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. *CoRL 2022*.

[^2]: Brohan, A., et al. (2023). RT-2: Vision-Language-Action Models. *arXiv:2307.15818*.

[^3]: Radford, A., et al. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. *OpenAI Technical Report*.
